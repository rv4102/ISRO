{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjDZk7vJJD1N"
      },
      "source": [
        "#Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "pGOJjgqYHnwG"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Loading data\n",
        "def image_with_bbox(img,input_txt_path):\n",
        "  with open(input_txt_path) as f:\n",
        "    bboxes = [line.split() for line in f.readlines()]\n",
        "  final_img = torch.zeros(img.shape,dtype=torch.float)\n",
        "  for bbox in bboxes:\n",
        "    class_name = bbox[0]\n",
        "    if class_name==2:\n",
        "      return None\n",
        "    y, x, w, h = map(float, bbox[1:])\n",
        "    x, y, h, w = map(int, (x * img.shape[-1], y * img.shape[-2], h * img.shape[-1], w*img.shape[-2]))\n",
        "    h = h//2\n",
        "    w = w//2\n",
        "    cropped_img = img.detach().numpy()[:,:,x-h:x+h,y-w:y+w]/255\n",
        "    final_img[:,:,x-h:x+h,y-w:y+w]=torch.from_numpy(cropped_img).type(torch.FloatTensor)\n",
        "  \n",
        "  return final_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "OatINR3PHTjk"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CosineSimilarity\n",
        "from torchvision.models import DenseNet201_Weights, densenet201\n",
        "\n",
        "def find_cosine_similarity(img1, img2):\n",
        "    \"\"\"\n",
        "    finds the cosine similarity between img1 and img2\n",
        "    \"\"\"\n",
        "\n",
        "    img1 = img1.squeeze(0)\n",
        "    img1 = torch.cat((img1,img1,img1),0) \n",
        "    img2 = img2.squeeze(0) \n",
        "    img2 = torch.cat((img2,img2,img2),0)\n",
        "    weights = DenseNet201_Weights.DEFAULT\n",
        "    model = densenet201(weights=weights)\n",
        "    model = model.cuda()\n",
        "    preprocess = weights.transforms()\n",
        "    newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))    \n",
        "\n",
        "    batch1 = preprocess(img1).unsqueeze(0)\n",
        "    pred1 = torch.flatten(newmodel(batch1).squeeze(0))\n",
        "    batch2 = preprocess(img2).unsqueeze(0)\n",
        "    pred2 = torch.flatten(newmodel(batch2).squeeze(0))\n",
        "\n",
        "    cos = CosineSimilarity(0)\n",
        "    return cos(pred1, pred2).item() # returns a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lQN7qe4utQ6",
        "outputId": "de585327-224e-42e7-b6ac-1a5927c5332e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ch2_tmc_ndn_20220510T1430273519_b_bot_d18_0.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_1.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_10.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_11.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_12.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_13.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_14.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_15.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_16.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_17.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_18.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_19.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_2.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_20.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_21.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_22.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_23.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_24.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_25.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_26.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_27.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_28.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_29.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_3.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_30.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_31.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_32.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_33.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_34.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_35.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_36.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_37.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_38.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_39.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_4.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_5.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_6.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_7.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_8.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_9.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_0.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_1.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_10.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_100.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_101.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_102.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_103.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_11.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_12.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_13.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_14.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_15.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_16.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_17.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_18.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_19.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_2.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_20.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_21.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_22.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_23.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_24.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_25.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_26.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_27.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_28.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_29.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_3.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_30.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_31.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_32.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_33.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_34.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_35.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_36.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_37.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_38.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_39.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_4.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_40.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_41.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_42.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_43.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_44.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_45.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_46.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_47.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_48.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_49.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_5.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_50.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_51.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_52.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_53.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_54.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_55.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_56.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_57.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_58.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_59.png']\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import PIL\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "files = sorted(os.listdir(\"/content/drive/MyDrive/Lunar_T-GAN/oth_low_res\"))[:100]\n",
        "print(files)\n",
        "path1=\"/content/drive/MyDrive/Lunar_T-GAN/oth_low_res/\"\n",
        "BOT_L = []\n",
        "for file in files:\n",
        "  img= np.array(Image.open(path1+file))\n",
        "  BOT_L.append(img)\n",
        "BOT_L = np.array(BOT_L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8NVBof2w58W",
        "outputId": "98ecc883-0175-4d02-bee2-8b4788e82d4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 250, 250, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "BOT_L.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym_dp6sYwj9q",
        "outputId": "346870cb-b3de-4fff-ec4e-345b3d0d1e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ch2_tmc_ndn_20220510T1430273519_b_bot_d18_0.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_1.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_10.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_11.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_12.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_13.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_14.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_15.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_16.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_17.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_18.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_19.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_2.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_20.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_21.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_22.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_23.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_24.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_25.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_26.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_27.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_28.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_29.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_3.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_30.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_31.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_32.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_33.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_34.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_35.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_36.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_37.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_38.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_39.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_4.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_5.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_6.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_7.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_8.png', 'ch2_tmc_ndn_20220510T1430273519_b_bot_d18_9.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_0.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_1.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_10.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_100.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_101.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_102.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_103.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_11.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_12.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_13.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_14.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_15.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_16.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_17.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_18.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_19.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_2.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_20.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_21.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_22.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_23.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_24.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_25.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_26.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_27.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_28.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_29.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_3.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_30.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_31.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_32.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_33.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_34.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_35.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_36.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_37.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_38.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_39.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_4.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_40.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_41.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_42.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_43.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_44.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_45.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_46.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_47.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_48.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_49.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_5.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_50.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_51.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_52.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_53.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_54.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_55.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_56.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_57.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_58.png', 'ch2_tmc_ndn_20220620T2127555941_b_bot_d32_59.png']\n"
          ]
        }
      ],
      "source": [
        "files = sorted(os.listdir(\"/content/drive/MyDrive/Lunar_T-GAN/oth\"))[:100]\n",
        "print(files)\n",
        "path1=\"/content/drive/MyDrive/Lunar_T-GAN/oth/\"\n",
        "BOT_H = []\n",
        "for file in files:\n",
        "  img= np.array(Image.open(path1+file))\n",
        "  BOT_H.append(img)\n",
        "BOT_H = np.array(BOT_H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEdysmJ9w8a-",
        "outputId": "85894872-47b1-476f-ca21-7908b8d1e0b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 1000, 1000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "BOT_H.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "tz5UkKC7RUBA"
      },
      "outputs": [],
      "source": [
        "check_files=files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "MIo531LsRYR6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KG7lnsDJJWr",
        "outputId": "398e9f72-72a9-4503-88a9-4518a729caad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "text_file=[]\n",
        "files = sorted(os.listdir(\"/content/drive/MyDrive/Lunar_T-GAN/crater\"))[:100]\n",
        "print(len(files))\n",
        "path1=\"/content/drive/MyDrive/Lunar_T-GAN/crater/\"\n",
        "for file in files:\n",
        "    text_file.append(path1+file)\n",
        "text_file = np.array(text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNKtVILuVpjv",
        "outputId": "caa845a3-cf00-4906-96f5-ae4c4cc090b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "text_file.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8dU3BGcKcG_",
        "outputId": "03924183-2c76-484b-ba9f-991dd30de2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_0.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_1.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_10.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_11.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_12.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_13.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_14.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_15.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_16.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_17.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_18.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_19.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_2.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_20.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_21.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_22.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_23.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_24.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_25.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_26.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_27.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_28.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_29.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_3.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_30.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_31.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_32.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_33.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_34.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_35.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_36.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_37.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_38.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_39.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_4.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_5.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_6.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_7.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_8.txt', 'ch2_tmc_ndn_20220510T1430273519_b_bdt_d18_9.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_0.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_1.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_10.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_100.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_101.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_102.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_103.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_11.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_12.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_13.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_14.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_15.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_16.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_17.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_18.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_19.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_2.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_20.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_21.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_22.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_23.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_24.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_25.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_26.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_27.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_28.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_29.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_3.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_30.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_31.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_32.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_33.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_34.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_35.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_36.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_37.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_38.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_39.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_4.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_40.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_41.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_42.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_43.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_44.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_45.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_46.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_47.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_48.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_49.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_5.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_50.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_51.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_52.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_53.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_54.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_55.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_56.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_57.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_58.txt', 'ch2_tmc_ndn_20220620T2127555941_b_bdt_d32_59.txt']\n"
          ]
        }
      ],
      "source": [
        "text_file2=[]\n",
        "files = sorted(os.listdir(\"/content/drive/MyDrive/Lunar_T-GAN/hills\"))[39:100+39]\n",
        "print(files)\n",
        "path1=\"/content/drive/MyDrive/Lunar_T-GAN/hills/\"\n",
        "for file in files:\n",
        "    text_file2.append(path1+file)\n",
        "text_file2 = np.array(text_file2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTxlLbKVMhan",
        "outputId": "512bcb26-8292-40de-9f45-8e60cb7d2a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        }
      ],
      "source": [
        "print(text_file2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2CDBwq7JIxX"
      },
      "source": [
        "#Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W_28ql_In3J",
        "outputId": "3d04f697-e48f-4bde-bc7c-72ba487b65e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\tres Loss: 0.3195\tD1 Loss: 0.6861\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.6896\n",
            "Epoch: 2\tres Loss: 0.3465\tD1 Loss: 0.7089\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.7010\n",
            "Epoch: 3\tres Loss: 0.3419\tD1 Loss: 0.6662\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.6797\n",
            "Epoch: 4\tres Loss: 0.3629\tD1 Loss: 0.0544\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3738\n",
            "Epoch: 5\tres Loss: 0.3758\tD1 Loss: 0.0034\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3483\n",
            "Epoch: 6\tres Loss: 0.3910\tD1 Loss: 0.0246\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3589\n",
            "Epoch: 7\tres Loss: 0.4018\tD1 Loss: 0.0018\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3475\n",
            "Epoch: 8\tres Loss: 0.4111\tD1 Loss: 0.0004\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3468\n",
            "Epoch: 9\tres Loss: 0.4232\tD1 Loss: 0.0001\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 10\tres Loss: 0.4316\tD1 Loss: 0.0001\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 11\tres Loss: 0.4400\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 12\tres Loss: 0.4485\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 13\tres Loss: 0.4554\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 14\tres Loss: 0.4622\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 15\tres Loss: 0.4685\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 16\tres Loss: 0.4740\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 17\tres Loss: 0.4786\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 18\tres Loss: 0.4831\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 19\tres Loss: 0.4849\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 20\tres Loss: 0.4875\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n",
            "Epoch: 21\tres Loss: 0.4896\tD1 Loss: 0.0000\tD2 Loss: 0.6931\tD3 Loss: 0.6931\tadv Loss: -0.3466\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# RVTT network (T1)\n",
        "class RVTT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = torch.sigmoid(self.deconv2(x))\n",
        "        return x\n",
        "\n",
        "# T1 discriminator\n",
        "class T1Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=4, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=4, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(256*16*16, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
        "        x = x.view(-1, 256*16*16)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# T2 discriminator\n",
        "class T2Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=4, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=4, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(256*16*16, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
        "        x = x.view(-1, 256*16*16)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# T3 discriminator\n",
        "class T3Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=4, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=4, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(256*16*16, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
        "        x = x.view(-1, 256*16*16)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "rvtt = RVTT()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "rvtt = rvtt.to(device)\n",
        "t1_discriminator = T1Discriminator()\n",
        "t1_discriminator = t1_discriminator.to(device) \n",
        "t2_discriminator = T2Discriminator()\n",
        "t2_discriminator = t2_discriminator.to(device)\n",
        "t3_discriminator = T3Discriminator()\n",
        "t3_discriminator = t3_discriminator.to(device)\n",
        "\n",
        "# Set loss function and optimizers\n",
        "criterion2 = nn.BCELoss()\n",
        "criterion1 = nn.MSELoss()\n",
        "res_optimizer = optim.Adam(rvtt.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d1_optimizer = optim.Adam(t1_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d2_optimizer = optim.Adam(t2_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d3_optimizer = optim.Adam(t3_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "adv_optimizer = optim.Adam(rvtt.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for data1,data2,text,text2 in zip(BOT_L,BOT_H,text_file,text_file2):\n",
        "        # Get training data\n",
        "        real_images1 = torch.from_numpy(data1[:,:,1]).to(device).unsqueeze(0).unsqueeze(3).type(torch.FloatTensor).permute(0,3,2,1).cuda()/255\n",
        "        real_images2 = torch.from_numpy(data2[:,:,1]).to(device).unsqueeze(0).unsqueeze(3).type(torch.FloatTensor).permute(0,3,2,1).cuda()/255\n",
        "        real_regions = text\n",
        "        # Train RVTT network\n",
        "        ## stage 1\n",
        "        res_optimizer.zero_grad()\n",
        "        rvtt  =  rvtt\n",
        "        fake_images = rvtt(real_images1)\n",
        "        per_loss=find_cosine_similarity(real_images2,fake_images)\n",
        "        res_loss = criterion1(real_images2, fake_images) +per_loss\n",
        "        res_loss.backward() \n",
        "        res_optimizer.step()\n",
        "\n",
        "        # Train T1 discriminator\n",
        "        ## stage 2\n",
        "        d1_optimizer.zero_grad()\n",
        "        real_discriminator_output = t1_discriminator(torch.cat((real_images2, fake_images.detach()), 1))\n",
        "        check1 = torch.ones((real_discriminator_output.size(0),real_discriminator_output.size(1)//2))\n",
        "        check2 = torch.zeros((real_discriminator_output.size(0),real_discriminator_output.size(1)//2))\n",
        "        check = torch.cat((check1,check2),1)\n",
        "  \n",
        "        d1_real_loss = criterion2(real_discriminator_output, check.cuda())\n",
        "        fake_discriminator_output = t1_discriminator(torch.cat((fake_images.detach(), real_images2), 1))\n",
        "        check = torch.cat((check2,check1),1)\n",
        "        \n",
        "        d1_fake_loss = criterion2(fake_discriminator_output, check.cuda())\n",
        "        d1_loss = 0.5*(d1_real_loss + d1_fake_loss)\n",
        "        d1_loss.backward(retain_graph=True)\n",
        "        d1_optimizer.step()\n",
        "\n",
        "\n",
        "        # Train T2 discriminator\n",
        "        ## stage 3\n",
        "        fake_images = rvtt(real_images1)\n",
        "        fake_regions = image_with_bbox(fake_images.cpu(),text)\n",
        "        real_regions = image_with_bbox(real_images2.cpu(),text)\n",
        "        \n",
        "        if fake_regions!=None:\n",
        "          \n",
        "          d2_optimizer.zero_grad()\n",
        "          real_discriminator_output = t2_discriminator(torch.cat((real_regions.cuda(), fake_regions.cuda()), 1))\n",
        "          check1 = torch.ones((real_discriminator_output.size(0),real_discriminator_output.size(1)//2))\n",
        "          check2 = torch.zeros((real_discriminator_output.size(0),real_discriminator_output.size(1)//2))\n",
        "          check = torch.cat((check1,check2),1)\n",
        "          d2_real_loss = criterion2(real_discriminator_output, check.cuda())\n",
        "          fake_discriminator_output = t2_discriminator(torch.cat((fake_regions.cuda(), real_regions.cuda()), 1))\n",
        "          check = torch.cat((check2,check1),1)\n",
        "          d2_fake_loss = criterion2(fake_discriminator_output, check.cuda())\n",
        "        \n",
        "          d2_loss = 0.5*(d2_real_loss + d2_fake_loss)\n",
        "          d2_loss.backward(retain_graph=True)\n",
        "          d2_optimizer.step()\n",
        "\n",
        "        # Train T3 discriminator\n",
        "        ##stage 4 \n",
        "        \n",
        "        fake_images = rvtt(real_images1)\n",
        "        fake_regions = image_with_bbox(fake_images.cpu(),text2)\n",
        "        real_regions = image_with_bbox(real_images2.cpu(),text2)\n",
        "        \n",
        "        if fake_regions!=None:\n",
        "        \n",
        "          d3_optimizer.zero_grad()\n",
        "          real_discriminator_output = t3_discriminator(torch.cat((real_regions.cuda(), fake_regions.cuda()), 1))\n",
        "          check1 = torch.ones((real_discriminator_output.size(0),real_discriminator_output.size(1)//2))\n",
        "          check2 = torch.zeros((real_discriminator_output.size(0),real_discriminator_output.size(1)//2))\n",
        "          check = torch.cat((check1,check2),1)\n",
        "          d3_real_loss = criterion2(real_discriminator_output, check.cuda())\n",
        "          fake_discriminator_output = t3_discriminator(torch.cat((fake_regions.cuda(), real_regions.cuda()), 1))\n",
        "          check = torch.cat((check2,check1),1)\n",
        "          d3_fake_loss = criterion2(fake_discriminator_output, check.cuda())\n",
        "          d3_loss = 0.5*(d3_real_loss + d3_fake_loss)\n",
        "          d3_loss.backward(retain_graph=True)\n",
        "          d3_optimizer.step()\n",
        "\n",
        "        # Train RVTT network\n",
        "        ## stage 5 \n",
        "\n",
        "          adv_optimizer.zero_grad()\n",
        "\n",
        "          real_discriminator_output = t1_discriminator(torch.cat((real_images2.detach(), fake_images.detach()), 1))\n",
        "          check = torch.cat((check1,check2),1)\n",
        "          d1_real_loss = criterion2(real_discriminator_output, check.cuda())\n",
        "          fake_discriminator_output = t1_discriminator(torch.cat((fake_images.detach(), real_images2.detach()), 1))\n",
        "          check = torch.cat((check2,check1),1)\n",
        "          d1_fake_loss = criterion2(fake_discriminator_output, check.cuda())\n",
        "        \n",
        "          d1_loss = 0.5*(d1_real_loss + d1_fake_loss)\n",
        "\n",
        "          real_discriminator_output = t2_discriminator(torch.cat((real_regions.cuda().detach(), fake_regions.cuda().detach()), 1))\n",
        "          check = torch.cat((check1,check2),1)\n",
        "          d2_real_loss = criterion2(real_discriminator_output, check.cuda())\n",
        "          fake_discriminator_output = t2_discriminator(torch.cat((fake_regions.cuda().detach(), real_regions.cuda().detach()), 1))\n",
        "          check = torch.cat((check2,check1),1)\n",
        "          d2_fake_loss = criterion2(fake_discriminator_output, check.cuda())\n",
        "        \n",
        "          d2_loss = 0.5*(d2_real_loss + d2_fake_loss)\n",
        "\n",
        "          real_discriminator_output = t3_discriminator(torch.cat((real_regions.cuda().detach(), fake_regions.cuda().detach()), 1))\n",
        "          check = torch.cat((check1,check2),1)\n",
        "          d3_real_loss = criterion2(real_discriminator_output, check.cuda())\n",
        "          fake_discriminator_output = t3_discriminator(torch.cat((fake_regions.cuda().detach(), real_regions.cuda().detach()), 1))\n",
        "          check = torch.cat((check2,check1),1)\n",
        "          d3_fake_loss = criterion2(fake_discriminator_output, check.cuda())\n",
        "        \n",
        "          d3_loss = 0.5*(d3_real_loss + d3_fake_loss)\n",
        "\n",
        "          adv_loss = -0.5*d1_loss- 0.25*d2_loss - 0.25*d3_loss\n",
        "          adv_loss.backward()\n",
        "          adv_optimizer.step()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print('Epoch: {}\\tres Loss: {:.4f}\\tD1 Loss: {:.4f}\\tD2 Loss: {:.4f}\\tD3 Loss: {:.4f}\\tadv Loss: {:.4f}'.format(\n",
        "            epoch + 1, res_loss.item(), d1_loss.item(), d2_loss.item(), d3_loss.item(),adv_loss.item() ))\n",
        "\n",
        "print('Training completed!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8TiG7Bh5elLP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjRMaiK3Pmxl"
      },
      "outputs": [],
      "source": [
        "torch.save(rvtt.state_dict(), \"/content/drive/MyDrive/Lunar_T-GAN/entire_model_new_try.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilkU2RXiDWjn",
        "outputId": "91471be8-0487-4da7-b174-31e77e937518"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "rvtt.load_state_dict(torch.load(\"/content/drive/MyDrive/Lunar_T-GAN/entire_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT-5-qMdEG52"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "img = cv2.imread(\"/content/test__.png\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "print(img.shape)\n",
        "img = torch.tensor(img).unsqueeze(0).unsqueeze(3).type(torch.FloatTensor).permute(0,3,2,1)/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-Tt-3MrIVXz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIkvjJ3SFw0R"
      },
      "outputs": [],
      "source": [
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nTLiDMbGHE-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le52E95DF6IU"
      },
      "outputs": [],
      "source": [
        "result = rvtt(img.cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmgvczA_F-DY"
      },
      "outputs": [],
      "source": [
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1udw2J1GBP2"
      },
      "outputs": [],
      "source": [
        "res=result.permute(0,3,2,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQkpIVbGGb1J"
      },
      "outputs": [],
      "source": [
        "res.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res=res.squeeze(0)\n",
        "res2=torch.cat((res,res,res),2)\n",
        "print(res2.shape)"
      ],
      "metadata": {
        "id": "RtWGOQLsitW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQDp2So2Ge3L"
      },
      "outputs": [],
      "source": [
        "cv2.imwrite(\"/content/finally.png\", res2.detach().cpu().numpy()*255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkrZVMQoGsG-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def ssim(img1, img2):\n",
        "    C1 = (0.01 * 255)**2\n",
        "    C2 = (0.03 * 255)**2\n",
        "\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
        "    window = np.outer(kernel, kernel.transpose())\n",
        "\n",
        "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n",
        "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
        "    mu1_sq = mu1**2\n",
        "    mu2_sq = mu2**2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
        "    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
        "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
        "                                                            (sigma1_sq + sigma2_sq + C2))\n",
        "    return ssim_map.mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7PqXq8WQOKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b1WHOHeeGaf"
      },
      "outputs": [],
      "source": [
        "def calculate_ssim(img1, img2):\n",
        "    '''calculate SSIM\n",
        "    '''\n",
        "    if not img1.shape == img2.shape:\n",
        "        raise ValueError('Input images must have the same dimensions.')\n",
        "    if img1.ndim == 2:\n",
        "        return ssim(img1, img2)\n",
        "    elif img1.ndim == 3:\n",
        "        if img1.shape[2] == 3:\n",
        "            ssims = []\n",
        "            for i in range(3):\n",
        "                ssims.append(ssim(img1, img2))\n",
        "            return np.array(ssims).mean()\n",
        "        elif img1.shape[2] == 1:\n",
        "            return ssim(np.squeeze(img1), np.squeeze(img2))\n",
        "    else:\n",
        "        raise ValueError('Wrong input image dimensions.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out=np.array(Image.open(\"/content/test4657.png\"))\n",
        "test = np.array(Image.open(\"/content/finally.png\"))\n",
        "calculate_ssim(out,test)"
      ],
      "metadata": {
        "id": "IizlNKuAQMCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKq1-eK7QkhN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}